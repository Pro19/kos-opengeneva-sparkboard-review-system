I am working on the following project:

Project title:
Ontology-Driven AI for Multi-Perspective Peer Review in Hackathons by Open Geneva Sparkboard

Project description:
We propose integrating AI with ontological frameworks to enhance the depth and
utility of peer review systems in hackathon environments. Rather than relying on
simplistic ranking scales, our approach would leverage structured knowledge
representation to capture both reviewer characteristics and feedback dimensions.
The ontology would classify reviewers by their professional domains (e.g.,
clinicians, administrators) and expertise levels, while simultaneously categorizing
feedback along multiple axes (technical feasibility, clinical impact, implementation
complexity). This structured approach enables aggregation and analysis capabilities - for
instance, automatically identifying when healthcare practitioners consistently
raise different concerns than technical reviewers, or detecting patterns in how
various stakeholder groups envision a project's trajectory. By combining this ontological
foundation with AI, we can generate nuanced, multi-perspective summaries of feedback
trends, making it easier to understand how different communities perceive and would
guide each project's development. This enhanced granularity in feedback analysis
could significantly improve project refinement and resource allocation decisions
in healthcare hackathons.


Current Implementation Idea:
- Use LLMs to generate ontology relating to hackathon, projects, reviewers, expertise, etc.
    - Refine ontology based on human feedback
- Get hackathon project descriptions from participants.
    - For each project:
    - Get textual project reviews + review confidence score (0-100) from human
      reviewers (review confidence score is used to determine if the reviewer is
      an expert, the idea is that experts will have high confidence in their reviews)
    - For each review:
        - Classify review (what domain, highly/lowly scored (semantic analysis))
        - Classify reviewer (expertise)
        - If 3rd party data (Google Scholar, LinkedIn, Github, etc.) are available, go to those 3rd party services and build a reviewer profile
            - Find reviewer domain (agriculture, physics, it, automotive, etc.)
            - Find reviewer domain irrelevance for current project (maybe use cosine similarity)
        - Filter out/reject review if the review is lowly scored and the domain
          is irrelevant (e.g. a low scored review from a calligraphy expert for
          a quantum physiscs related project)
    - Aggregate accepted reviews by domain
    - For missing domains, create artificial reviews from LLMs (create prompt like
      "given this project, pretend to be an expert from X field and give a text review") (use definitions from ontology to create specific prompts for domains)
    - Based on all accepted reviews (human + artificial), generate feedback scores
      in various areas (project novelty, feasibility, ROI, etc.)
    - Based on the calculated feedback score + accepted reviews, generate a final textual feedback review
    - Show feedback scores and feedback review
    - Update ontology with new domains/updated definitions based on the project description
        - If an human expert reviewer is identified make them update the ontology


```
projects/
    |- project-1/
        |- description.md
        |- review1.md
        |- review2.md
    |- project-2/
        |- description.md
        |- review3.md
        |- review4.md
        |- review5.md
```

description.md
```
# Hackathon ID:

## Project Name:

## Project Description (max 400 words):

## Explain the work you've done so far:
```


human-review1.md
```
# Reviewer name

## Links
- LinkedIn :
- Google Scholar :
- Github :

## Text review of the project (max 400 words):

## Confidence score (0-100) _How much confidence do you have in your own review?_: